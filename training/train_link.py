import os
import time
import torch
import torch.nn as nn
import dgl
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.tensorboard import SummaryWriter
import sys
import psutil
import GPUtil
import torch.multiprocessing as mp

# ==========================================================
# æ·»åŠ å½“å‰ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
# ==========================================================
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from models.gcn import GCN
from datasets.data_generator import GraphGenerator
import datetime


# ==========================================================
# 1ï¸âƒ£ åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ
# ==========================================================
def setup_distributed(rank, world_size):
    """
    åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ
    
    å…³é”®å‚æ•°è®¾ç½®:
    - MASTER_ADDR: ä¸»èŠ‚ç‚¹åœ°å€ï¼Œé€šå¸¸ä¸ºlocalhost
    - MASTER_PORT: é€šä¿¡ç«¯å£
    - backend: é€šä¿¡åç«¯ï¼ŒGPUæ¨èä½¿ç”¨nccl
    """
    os.environ["MASTER_ADDR"] = "127.0.0.1"
    os.environ["MASTER_PORT"] = "29500"
    os.environ["RANK"] = str(rank)
    os.environ["WORLD_SIZE"] = str(world_size)
    
    torch.cuda.set_device(rank)
    dist.init_process_group(backend="nccl", 
                            rank=rank, 
                            world_size=world_size)
    device = torch.device(f"cuda:{rank}")
    
    print(f"ğŸš€ Rank {rank}: åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ")
    return device





# ==========================================================
# 2ï¸âƒ£ å›¾ç”Ÿæˆä¸åˆ’åˆ†
# ==========================================================
def prepare_graph(graph_dir="datasets/graph_parts", num_parts=3, nodes=20):
    """
    ç”Ÿæˆæˆ–åŠ è½½åˆ’åˆ†å¥½çš„å›¾æ•°æ®
    
    å…³é”®å‚æ•°è®¾ç½®:
    - graph_dir: å›¾æ•°æ®å­˜å‚¨ç›®å½•
    - num_parts: å›¾åˆ’åˆ†ä»½æ•°ï¼Œå¯¹åº”åˆ†å¸ƒå¼è¿›ç¨‹æ•°
    - nodes: å›¾èŠ‚ç‚¹æ•°é‡
    - p: ERå›¾ç”Ÿæˆæ¦‚ç‡
    """
    if not os.path.exists(graph_dir):
        print(f"ğŸ”§ ç”Ÿæˆæ–°çš„å›¾æ•°æ®å¹¶åˆ’åˆ†...")
        os.makedirs(graph_dir, exist_ok=True)
        gen = GraphGenerator()
        G_nx = gen.generate_nx_graph(kind='ER', n_nodes=nodes, p=0.01)
        g = gen.nx_to_dgl(G_nx)
        gen.add_node_labels(g)

        # ä½¿ç”¨metisç®—æ³•è¿›è¡Œå›¾åˆ’åˆ†
        gen.partition_graph(g, num_parts=num_parts, method='metis', output_dir=graph_dir)

        print(f"âœ… å›¾æ•°æ®ç”Ÿæˆå’Œåˆ’åˆ†å®Œæˆ")

    return graph_dir


def train_fn(rank, world_size, graph_dir, num_epochs=100, lr=0.001):
    """
    åˆ†å¸ƒå¼è®­ç»ƒå‡½æ•°ï¼Œç”±mp.spawnè°ƒç”¨
    
    å…³é”®å‚æ•°è®¾ç½®:
    - num_epochs: è®­ç»ƒè½®æ•°
    - lr: å­¦ä¹ ç‡
    """
    device = setup_distributed(rank, world_size)
    local_rank = rank
    print(f"Rank {rank} å¯åŠ¨è®­ç»ƒè¿›ç¨‹")
    # è°ƒç”¨ä¸»è®­ç»ƒå‡½æ•°
    train(rank, local_rank, world_size, device, graph_dir=graph_dir, num_epochs=num_epochs, lr=lr)
    
    dist.destroy_process_group()

# ==========================================================
# 3ï¸âƒ£ è®­ç»ƒå‡½æ•°
# ==========================================================
def train(rank, local_rank, world_size, device, graph_dir, num_epochs=20, lr=0.001, partition_method="metis"):
    """
    é“¾è·¯é¢„æµ‹è®­ç»ƒä¸»å‡½æ•°
    
    å…³é”®å‚æ•°è®¾ç½®:
    - num_epochs: è®­ç»ƒè½®æ•°
    - lr: å­¦ä¹ ç‡
    - partition_method: å›¾åˆ’åˆ†æ–¹æ³•
    - batch_size: æ‰¹æ¬¡å¤§å°
    - train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
    - sampler_fanouts: é‚»å±…é‡‡æ ·å±‚æ•°é…ç½®
    - input_dim/hidden_dim/output_dim: æ¨¡å‹ç»´åº¦é…ç½®
    """
    torch.manual_seed(0)

    # åˆå§‹åŒ– TensorBoardï¼Œä»… rank 0 å†™æ—¥å¿—
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    writer = SummaryWriter(log_dir=f"runs/link_prediction_{timestamp}_rank{rank}") if rank == 0 else None

    # æ•°æ®åŠ è½½å™¨åˆå§‹åŒ– - æ¯ä¸ªrankåŠ è½½å¯¹åº”çš„å­å›¾åˆ†åŒº
    gen = GraphGenerator()
    train_loader, test_loader, subg = gen.get_dataloader_for_link_prediction(
        pid=rank,  # ä½¿ç”¨rankä½œä¸ºåˆ†åŒºID
        partition_method=partition_method,
        batch_size=32,  # å…³é”®å‚æ•°: æ‰¹æ¬¡å¤§å°
        train_ratio=0.8,  # å…³é”®å‚æ•°: è®­ç»ƒé›†æ¯”ä¾‹
        num_workers=0,
        device=device,
        sampler_fanouts=[10, 10, 5],  # å…³é”®å‚æ•°: é‚»å±…é‡‡æ ·é…ç½®
        partition_dir=graph_dir
    )
    print(f"Batch size: {train_loader.batch_size}")
    num_batches = len(train_loader)
    print(f"æ¯ä¸ª epoch éœ€è¦è¿­ä»£ {num_batches} æ¬¡")
    if rank == 0:
        print(f"ğŸ“Š Rank {rank} åŠ è½½å®Œæˆ dataloaderï¼ˆå­å›¾ {rank}ï¼‰")
    subg = subg.to(device)
    # æ¨¡å‹åˆå§‹åŒ–
    input_dim = 4   # å…³é”®å‚æ•°: è¾“å…¥ç‰¹å¾ç»´åº¦
    hidden_dim = 64  # å…³é”®å‚æ•°: éšè—å±‚ç»´åº¦
    output_dim = 1   # å…³é”®å‚æ•°: è¾“å‡ºç»´åº¦(é“¾è·¯é¢„æµ‹å¾—åˆ†)
    
    gnn = GCN(input_dim, hidden_dim, output_dim, dropout=0.0).to(device)
    gnn = DDP(gnn, device_ids=[local_rank], output_device=local_rank)
    optimizer = torch.optim.Adam(gnn.parameters(), lr=lr)

    if rank == 0:
        print(f"ğŸš€ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ: {sum(p.numel() for p in gnn.parameters()):,} å‚æ•°")

    # ============ å¼€å§‹è®­ç»ƒ ============
    gnn.train()
    prev_comm_time = 0

    # åŒæ­¥å„è¿›ç¨‹çš„æ‰¹æ¬¡æ•°é‡ï¼Œç¡®ä¿åˆ†å¸ƒå¼è®­ç»ƒåŒæ­¥
    local_num_batches = len(train_loader)
    num_batches_tensor = torch.tensor(local_num_batches, device=device)
    all_num_batches = [torch.zeros_like(num_batches_tensor) for _ in range(world_size)]
    dist.all_gather(all_num_batches, num_batches_tensor)
    max_num_batches = max([x.item() for x in all_num_batches])
    
    # ============ è®­ç»ƒå¾ªç¯ ============
    for epoch in range(num_epochs):
        epoch_start_time = time.time()
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        epoch_comm_time = 0
        epoch_forward_time = 0
        epoch_backward_time = 0
        
        iter = 1
        if iter <= local_num_batches:
            for input_nodes, pos_pair_graph, neg_pair_graph, blocks in train_loader:
                
                comm_start_time = time.time()

                # 1ï¸âƒ£ å–å‡ºèŠ‚ç‚¹ç‰¹å¾ï¼ˆè¾“å…¥å±‚æºèŠ‚ç‚¹ï¼‰
                feats = subg.ndata['feat'][input_nodes].to(device)
                # feats = blocks[0].srcdata["feat"].to(device)

                # 2ï¸âƒ£ ä½¿ç”¨ blocks åš GCN å‰å‘ç¼–ç ï¼ˆmessage passingï¼‰
                # gnn çš„ forward éœ€è¦ (blocks, feats)
                node_emb = gnn(blocks, feats)   # è¾“å‡ºçš„æ˜¯ç›®æ ‡èŠ‚ç‚¹çš„embeddingï¼ˆæœ€åä¸€å±‚blockçš„dstèŠ‚ç‚¹ï¼‰

                comm_end_time = time.time()
                epoch_comm_time += (comm_end_time - comm_start_time)
                
                # 3ï¸âƒ£ ä»æ­£æ ·æœ¬å›¾ä¸­å–å‡ºè¾¹ä¸¤ç«¯èŠ‚ç‚¹çš„ embedding
                pos_src, pos_dst = pos_pair_graph.edges()
                pos_src_emb = node_emb[pos_src]
                pos_dst_emb = node_emb[pos_dst]
                # ç‚¹ä¹˜å¾—åˆ†
                pos_score = (pos_src_emb * pos_dst_emb).sum(dim=1)

                # 4ï¸âƒ£ ä»è´Ÿæ ·æœ¬å›¾ä¸­å–å‡ºè¾¹ä¸¤ç«¯èŠ‚ç‚¹çš„ embedding
                neg_src, neg_dst = neg_pair_graph.edges()
                neg_src_emb = node_emb[neg_src]
                neg_dst_emb = node_emb[neg_dst]
                neg_score = (neg_src_emb * neg_dst_emb).sum(dim=1)
                
                # 5ï¸âƒ£ è®¡ç®—é“¾è·¯é¢„æµ‹æŸå¤±ï¼ˆè´Ÿæ ·æœ¬æ ‡ç­¾ä¸º 0ï¼Œæ­£æ ·æœ¬ä¸º 1ï¼‰
                scores = torch.cat([pos_score, neg_score], dim=0)
                labels = torch.cat([
                    torch.ones_like(pos_score),
                    torch.zeros_like(neg_score)
                ]).to(device)

                forward_start = time.time()
                loss = nn.functional.binary_cross_entropy_with_logits(scores, labels)
                forward_end = time.time()
                epoch_forward_time += (forward_end - forward_start)
                
                # 6ï¸âƒ£ åå‘ä¼ æ’­ä¸ä¼˜åŒ–
                backward_start = time.time()
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                backward_end = time.time()
                epoch_backward_time += (backward_end - backward_start)

                # è®¡ç®—å‡†ç¡®åº¦
                predictions = (torch.sigmoid(scores) > 0.5).float()
                accuracy = (predictions == labels).float().mean().item()
                
                # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
                process = psutil.Process()
                memory_usage = process.memory_info().rss / 1024 / 1024  # MB
                
                # è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                gpu_memory_usage = 0
                try:
                    gpus = GPUtil.getGPUs()
                    if gpus:
                        gpu_memory_usage = gpus[local_rank].memoryUsed
                except:
                    pass

                total_loss += loss.item()
                total_accuracy += accuracy
                num_batches += 1
                
                if num_batches % 10 == 0:
                    print(f"Rank {rank} batch {num_batches} å†…è®­ç»ƒä¸­, loss: {loss.item():.4f}, acc: {accuracy:.4f}")
                iter += 1
        
        # 7ï¸âƒ£ åŒæ­¥å…¶ä»–è¿›ç¨‹çš„æ‰¹æ¬¡è®­ç»ƒï¼ˆç¡®ä¿åˆ†å¸ƒå¼è®­ç»ƒåŒæ­¥ï¼‰
        if iter > local_num_batches:
            for _ in range(max_num_batches - local_num_batches):
                dist.all_reduce(torch.zeros(1, device=device))

        # ============ è®¡ç®—è®­ç»ƒæŒ‡æ ‡ ============
        # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®åº¦
        avg_loss = total_loss / num_batches
        avg_accuracy = total_accuracy / num_batches

        # åŒæ­¥æ‰€æœ‰è¿›ç¨‹çš„æŸå¤±å’Œå‡†ç¡®åº¦
        avg_loss_tensor = torch.tensor(avg_loss, device=device)
        avg_accuracy_tensor = torch.tensor(avg_accuracy, device=device)
        
        dist.all_reduce(avg_loss_tensor, op=dist.ReduceOp.SUM)
        dist.all_reduce(avg_accuracy_tensor, op=dist.ReduceOp.SUM)
        
        avg_loss_tensor /= world_size
        avg_accuracy_tensor /= world_size

        # è®°å½•è®­ç»ƒæ—¶é—´
        epoch_time = time.time() - epoch_start_time

        # ============ æ—¥å¿—è®°å½• ============
        # è®°å½•åˆ° TensorBoardï¼ˆä»…åœ¨ rank 0 è¿›ç¨‹è®°å½•ï¼‰
        if rank == 0:
            writer.add_scalar("Loss/train", avg_loss_tensor.item(), epoch)
            writer.add_scalar("Accuracy/train", avg_accuracy_tensor.item(), epoch)
            writer.add_scalar("Time/epoch", epoch_time, epoch)
            writer.add_scalar("Time/communication", epoch_comm_time, epoch)
            writer.add_scalar("Time/forward", epoch_forward_time, epoch)
            writer.add_scalar("Time/backward", epoch_backward_time, epoch)
            writer.add_scalar("Memory/usage", memory_usage, epoch)

        # æ‰“å°è®­ç»ƒä¿¡æ¯
        if rank == 0:
            print(f"Epoch {epoch+1}/{num_epochs}, "
                  f"Loss: {avg_loss_tensor.item():.4f}, "
                  f"Accuracy: {avg_accuracy_tensor.item():.4f}, "
                  f"Time: {epoch_time:.2f}s")

        # # ============ æµ‹è¯•è¯„ä¼° ============
        # # æ¯ 10 ä¸ª epoch è¿›è¡Œä¸€æ¬¡æµ‹è¯•è¯„ä¼°
        # if (epoch + 1) % 10 == 0:
        #     test_accuracy = evaluate(gnn, test_loader, device, rank, world_size)
        #     if rank == 0:
        #         writer.add_scalar("Accuracy/test", test_accuracy, epoch)
        #         print(f"Test Accuracy at epoch {epoch+1}: {test_accuracy:.4f}")

    # ============ æ¨¡å‹ä¿å­˜ ============
    # ä¿å­˜æ¨¡å‹ï¼ˆä»…åœ¨ rank 0 è¿›ç¨‹ä¿å­˜ï¼‰
    if rank == 0:
        torch.save(gnn.state_dict(), f"link_prediction_model_rank{rank}.pth")
        print(f"Model saved as link_prediction_model_rank{rank}.pth")

    # ============ æœ€ç»ˆæµ‹è¯•è¯„ä¼° ============
    # åœ¨æ‰€æœ‰è®­ç»ƒç»“æŸåè¿›è¡Œå®Œæ•´çš„æµ‹è¯•è¯„ä¼°
    final_test_accuracy = evaluate(gnn, test_loader, device, rank, world_size)
    
    # æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„æµ‹è¯•å‡†ç¡®åº¦
    test_acc_tensor = torch.tensor(final_test_accuracy, device=device)
    all_test_acc = [torch.zeros_like(test_acc_tensor) for _ in range(world_size)]
    dist.all_gather(all_test_acc, test_acc_tensor)
    
    # è®¡ç®—å¹³å‡æµ‹è¯•å‡†ç¡®åº¦
    avg_test_accuracy = sum([acc.item() for acc in all_test_acc]) / world_size
    
    if rank == 0:
        print(f"\nğŸ“Š æœ€ç»ˆæµ‹è¯•ç»“æœ:")
        print(f"å„è¿›ç¨‹æµ‹è¯•å‡†ç¡®åº¦: {[acc.item() for acc in all_test_acc]}")
        print(f"å¹³å‡æµ‹è¯•å‡†ç¡®åº¦: {avg_test_accuracy:.4f}")
        writer.add_scalar("Accuracy/final_test", avg_test_accuracy, num_epochs)

    # å…³é—­ TensorBoard writer
    if rank == 0:
        writer.close()

    print(f"Rank {rank} training completed.")


# ==========================================================
# 5ï¸âƒ£ è¯„ä¼°å‡½æ•°
# ==========================================================
def evaluate(model, test_loader, device, rank, world_size):
    """
    æ¨¡å‹è¯„ä¼°å‡½æ•°
    
    å…³é”®å‚æ•°è®¾ç½®:
    - model: å¾…è¯„ä¼°çš„GNNæ¨¡å‹
    - test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
    - device: è®¡ç®—è®¾å¤‡
    - rank: å½“å‰è¿›ç¨‹æ’å
    - world_size: è¿›ç¨‹æ€»æ•°
    """
    model.eval()
    total_accuracy = 0.0
    num_batches = 0
    
    with torch.no_grad():
        for input_nodes, pos_pair_graph, neg_pair_graph, blocks in test_loader:
            # 1ï¸âƒ£ è·å–èŠ‚ç‚¹ç‰¹å¾
            feats = blocks[0].srcdata["feat"].to(device)
            
            # 2ï¸âƒ£ GCNå‰å‘ä¼ æ’­è·å–èŠ‚ç‚¹åµŒå…¥
            node_emb = model(blocks, feats)
            
            # 3ï¸âƒ£ è®¡ç®—æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„é¢„æµ‹å¾—åˆ†
            pos_src, pos_dst = pos_pair_graph.edges()
            pos_src_emb = node_emb[pos_src]
            pos_dst_emb = node_emb[pos_dst]
            pos_score = (pos_src_emb * pos_dst_emb).sum(dim=1)
            
            neg_src, neg_dst = neg_pair_graph.edges()
            neg_src_emb = node_emb[neg_src]
            neg_dst_emb = node_emb[neg_dst]
            neg_score = (neg_src_emb * neg_dst_emb).sum(dim=1)
            
            # 4ï¸âƒ£ åˆå¹¶æ­£è´Ÿæ ·æœ¬å¾—åˆ†å’Œæ ‡ç­¾
            scores = torch.cat([pos_score, neg_score], dim=0)
            labels = torch.cat([
                torch.ones_like(pos_score),
                torch.zeros_like(neg_score)
            ]).to(device)
            
            # 5ï¸âƒ£ è®¡ç®—å‡†ç¡®åº¦
            predictions = (torch.sigmoid(scores) > 0.5).float()
            accuracy = (predictions == labels).float().mean().item()
            
            total_accuracy += accuracy
            num_batches += 1
    
    # 6ï¸âƒ£ åŒæ­¥æ‰€æœ‰è¿›ç¨‹çš„å‡†ç¡®åº¦
    avg_accuracy = total_accuracy / num_batches if num_batches > 0 else 0.0
    avg_accuracy_tensor = torch.tensor(avg_accuracy, device=device)
    dist.all_reduce(avg_accuracy_tensor, op=dist.ReduceOp.SUM)
    avg_accuracy_tensor /= world_size
    
    model.train()
    return avg_accuracy_tensor.item()


# ==========================================================
# 6ï¸âƒ£ ä¸»å…¥å£
# ==========================================================
if __name__ == "__main__":
    """
    ä¸»ç¨‹åºå…¥å£
    
    å…³é”®å‚æ•°è®¾ç½®:
    - graph_dir: å›¾æ•°æ®å­˜å‚¨ç›®å½•
    - num_parts: å›¾åˆ’åˆ†ä»½æ•°ï¼Œå¯¹åº”åˆ†å¸ƒå¼è¿›ç¨‹æ•°
    - nodes: å›¾èŠ‚ç‚¹æ•°é‡
    - world_size: åˆ†å¸ƒå¼è¿›ç¨‹æ•°é‡
    """
    # å…³é”®å‚æ•°: å›¾æ•°æ®é…ç½®
    graph_dir = prepare_graph(graph_dir="datasets/link_prediction_ER", num_parts=3, nodes=1000)
    world_size = 3
    device = None
    
    # ä½¿ç”¨mp.spawnå¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
    mp.spawn(
        train_fn,
        args=(world_size, graph_dir),
        nprocs=world_size,
        join=True
    )
