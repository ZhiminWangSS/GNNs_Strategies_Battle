import os
import sys
import time
import torch
import torch.nn as nn
import torch.distributed as dist
import torch.multiprocessing as mp
import datetime
from torch.nn.parallel import DistributedDataParallel as DDP
import dgl
from torch.utils.tensorboard import SummaryWriter



sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.gcn import GCN
from datasets.data_generator import GraphGenerator



class CommunicationMonitor:
    def __init__(self):
        self.total_bytes = 0
        self.start_time = None
        self.end_time = None

    def start(self):
        self.total_bytes = 0
        self.start_time = time.time()

    def hook(self, tensor):
        """æ¯æ¬¡é€šä¿¡è‡ªåŠ¨ç»Ÿè®¡æ•°æ®é‡"""
        self.total_bytes += tensor.numel() * tensor.element_size()
        return tensor

    def stop(self):
        self.end_time = time.time()

    def get_bandwidth(self):
        if self.start_time is None or self.end_time is None:
            return 0.0, 0.0
        duration = max(1e-6, self.end_time - self.start_time)
        kb = self.total_bytes / (1024)
        return kb, kb / duration  # (é€šä¿¡é‡ KB, å¸¦å®½ KB/s)


# ==========================================================
# 1ï¸âƒ£ åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
# ==========================================================
def setup_distributed(rank, world_size):
    os.environ["MASTER_ADDR"] = "127.0.0.1"
    os.environ["MASTER_PORT"] = "29501"
    torch.cuda.set_device(rank)
    dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)
    device = torch.device(f"cuda:{rank}")
    print(f"ğŸš€ Rank {rank}: åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ")
    return device


# ==========================================================
# ğŸ” é€šä¿¡ç»Ÿè®¡æ¨¡å—
# ==========================================================









# ==========================================================
# 2ï¸âƒ£ å›¾ç”Ÿæˆä¸åˆ’åˆ†
# ==========================================================
def prepare_graph(graph_dir="datasets/graph_parts", num_parts=3, nodes=20):
    """
    ç”Ÿæˆæˆ–åŠ è½½åˆ’åˆ†å¥½çš„å›¾æ•°æ®
    
    å…³é”®å‚æ•°è®¾ç½®:
    - graph_dir: å›¾æ•°æ®å­˜å‚¨ç›®å½•
    - num_parts: å›¾åˆ’åˆ†ä»½æ•°ï¼Œå¯¹åº”åˆ†å¸ƒå¼è¿›ç¨‹æ•°
    - nodes: å›¾èŠ‚ç‚¹æ•°é‡
    - p: ERå›¾ç”Ÿæˆæ¦‚ç‡
    """
    if not os.path.exists(graph_dir):
        print(f"ğŸ”§ ç”Ÿæˆæ–°çš„å›¾æ•°æ®å¹¶åˆ’åˆ†...")
        os.makedirs(graph_dir, exist_ok=True)
        gen = GraphGenerator()
        G_nx = gen.generate_nx_graph(kind='ER', n_nodes=nodes, p=0.01)
        g = gen.nx_to_dgl(G_nx)
        gen.add_node_labels(g)

        # ä½¿ç”¨metisç®—æ³•è¿›è¡Œå›¾åˆ’åˆ†
        gen.partition_graph(g, num_parts=num_parts, method='metis', output_dir=graph_dir)

        print(f"âœ… å›¾æ•°æ®ç”Ÿæˆå’Œåˆ’åˆ†å®Œæˆ")

    return graph_dir


def train_fn(rank, world_size, graph_dir, num_epochs=20, lr=0.001):
    """
    åˆ†å¸ƒå¼è®­ç»ƒå‡½æ•°ï¼Œç”±mp.spawnè°ƒç”¨
    
    å…³é”®å‚æ•°è®¾ç½®:
    - num_epochs: è®­ç»ƒè½®æ•°
    - lr: å­¦ä¹ ç‡
    """
    device = setup_distributed(rank, world_size)
    local_rank = rank
    print(f"Rank {rank} å¯åŠ¨è®­ç»ƒè¿›ç¨‹")
    # è°ƒç”¨ä¸»è®­ç»ƒå‡½æ•°
    train(rank, local_rank, world_size, device, graph_dir=graph_dir, num_epochs=num_epochs, lr=lr)
    
    dist.destroy_process_group()

# ==========================================================
# 3ï¸âƒ£ è®­ç»ƒå‡½æ•°
# ==========================================================
def train(rank, local_rank, world_size, device, graph_dir, num_epochs=20, lr=0.001, partition_method="metis"):
    """
    é“¾è·¯é¢„æµ‹è®­ç»ƒä¸»å‡½æ•°
    
    å…³é”®å‚æ•°è®¾ç½®:
    - num_epochs: è®­ç»ƒè½®æ•°
    - lr: å­¦ä¹ ç‡
    - partition_method: å›¾åˆ’åˆ†æ–¹æ³•
    - batch_size: æ‰¹æ¬¡å¤§å°
    - train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
    - sampler_fanouts: é‚»å±…é‡‡æ ·å±‚æ•°é…ç½®
    - input_dim/hidden_dim/output_dim: æ¨¡å‹ç»´åº¦é…ç½®
    """
    torch.manual_seed(0)

    # åˆå§‹åŒ– TensorBoardï¼Œä»… rank 0 å†™æ—¥å¿—
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    writer = SummaryWriter(log_dir=f"runs/link_prediction_{timestamp}_rank{rank}") if rank == 0 else None

    # æ•°æ®åŠ è½½å™¨åˆå§‹åŒ– - æ¯ä¸ªrankåŠ è½½å¯¹åº”çš„å­å›¾åˆ†åŒº
    gen = GraphGenerator()
    train_loader, test_loader, subg = gen.get_dataloader_for_link_prediction(
        pid=rank,  # ä½¿ç”¨rankä½œä¸ºåˆ†åŒºID
        partition_method=partition_method,
        batch_size=32,  # å…³é”®å‚æ•°: æ‰¹æ¬¡å¤§å°
        train_ratio=0.8,  # å…³é”®å‚æ•°: è®­ç»ƒé›†æ¯”ä¾‹
        num_workers=0,
        device=device,
        sampler_fanouts=[10, 10, 5],  # å…³é”®å‚æ•°: é‚»å±…é‡‡æ ·é…ç½®
        partition_dir=graph_dir
    )
    # print(f"Batch size: {train_loader.batch_size}")
    num_batches = len(train_loader)
    # print(f"æ¯ä¸ª epoch éœ€è¦è¿­ä»£ {num_batches} æ¬¡")
    # if rank == 0:
    #     print(f"ğŸ“Š Rank {rank} åŠ è½½å®Œæˆ dataloaderï¼ˆå­å›¾ {rank}ï¼‰")
    subg = subg.to(device)
    # æ¨¡å‹åˆå§‹åŒ–
    input_dim = 4   # å…³é”®å‚æ•°: è¾“å…¥ç‰¹å¾ç»´åº¦
    hidden_dim = 64  # å…³é”®å‚æ•°: éšè—å±‚ç»´åº¦
    output_dim = 32   # å…³é”®å‚æ•°: è¾“å‡ºç»´åº¦(é“¾è·¯é¢„æµ‹å¾—åˆ†)
    
    model = GCN(input_dim, hidden_dim, output_dim, dropout=0.0).to(device)
    model = DDP(model, device_ids=[local_rank], output_device=local_rank)
    
    comm_monitor = CommunicationMonitor()
    def communication_hook(state, bucket: dist.GradBucket):
        tensor = bucket.buffer()
        comm_monitor.hook(tensor)
        fut = dist.all_reduce(tensor, async_op=True).get_future()
        # print(f"[Rank {rank}] Hook triggered with tensor size {tensor.numel()} ({tensor.numel() * tensor.element_size()/1024:.2f} KB)")
        return fut.then(lambda fut: fut.value()[0])
    # æ³¨å†Œé€šä¿¡hookä»¥ç›‘æ§é€šä¿¡é‡
    model.register_comm_hook(state=None, hook=communication_hook)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    if rank == 0:
        print(f"ğŸš€ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ: {sum(p.numel() for p in model.parameters()):,} å‚æ•°")

    # ======================================================
    # ğŸ” è®­ç»ƒå¾ªç¯
    # ======================================================
    model.train()
    
    # åŒæ­¥å„è¿›ç¨‹çš„æ‰¹æ¬¡æ•°é‡ï¼Œç¡®ä¿åˆ†å¸ƒå¼è®­ç»ƒåŒæ­¥
    local_num_batches = len(train_loader)
    num_batches_tensor = torch.tensor(local_num_batches, device=device)
    all_num_batches = [torch.zeros_like(num_batches_tensor) for _ in range(world_size)]
    dist.all_gather(all_num_batches, num_batches_tensor)
    max_num_batches = max([x.item() for x in all_num_batches])
    
    for epoch in range(num_epochs):
        model.train()
        total_loss, total_acc = 0.0, 0.0
        num_batches = 0
        epoch_forward, epoch_backward, epoch_comm, epoch_batch = 0.0, 0.0, 0.0, 0.0
        epoch_comm_kb, epoch_comm_bw = 0.0, 0.0
        iter = 1
        if iter <= local_num_batches:
            for input_nodes, pos_pair_graph, neg_pair_graph, blocks in train_loader:
                batch_start = time.time()
                # ============ æ•°æ®å‡†å¤‡ ============
                feats = subg.ndata['feat'][input_nodes].to(device)
                
                # ============ å‰å‘ä¼ æ’­ ============
                forward_start = time.time()
                node_emb = model(blocks, feats)   # è¾“å‡ºç›®æ ‡èŠ‚ç‚¹çš„embedding
                
                
                # æ­£æ ·æœ¬embeddingæå–å’Œå¾—åˆ†è®¡ç®—
                pos_src, pos_dst = pos_pair_graph.edges()
                pos_src_emb = node_emb[pos_src]
                pos_dst_emb = node_emb[pos_dst]
                pos_score = (pos_src_emb * pos_dst_emb).sum(dim=1)
                
                
                # è´Ÿæ ·æœ¬embeddingæå–å’Œå¾—åˆ†è®¡ç®—
                neg_src, neg_dst = neg_pair_graph.edges()
                neg_src_emb = node_emb[neg_src]
                neg_dst_emb = node_emb[neg_dst]
                neg_score = (neg_src_emb * neg_dst_emb).sum(dim=1)
                
                # è®¡ç®—é“¾è·¯é¢„æµ‹æŸå¤±
                scores = torch.cat([pos_score, neg_score], dim=0)
                labels = torch.cat([
                    torch.ones_like(pos_score),
                    torch.zeros_like(neg_score)
                ]).to(device)
                loss = nn.functional.binary_cross_entropy_with_logits(scores, labels)
                forward_time = (time.time() - forward_start)
                
                # ============ åå‘ä¼ æ’­ä¸ä¼˜åŒ– ============
                backward_start = time.time()
                optimizer.zero_grad()
                comm_monitor.start()
                loss.backward()
                comm_monitor.stop()
                optimizer.step()
                batch_time = time.time() - batch_start
                backward_time = (time.time() - backward_start)

                # è®¡ç®—å‡†ç¡®åº¦
                predictions = (torch.sigmoid(scores) > 0.5).float()
                acc = (predictions == labels).float().mean().item()
                
                
                
                # è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                gpu_memory_allocated = 0.0
                gpu_memory_reserved = 0.0
                gpu_utilization = 0.0
                try:
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()
                        gpu_memory_allocated = torch.cuda.memory_allocated(device) / (1024**2)  # GB
                        gpu_memory_reserved = torch.cuda.memory_reserved(device) / (1024**2)  # GB
                        gpus = GPUtil.getGPUs()
                        if gpus:
                            gpu_utilization = gpus[local_rank].load * 100  # %
                except:
                    pass
                
                total_loss += loss.item()
                total_acc += acc
                num_batches += 1
                epoch_forward += forward_time
                epoch_backward += backward_time
                epoch_batch += batch_time
                # epoch_comm_mb += comm_mb
                # epoch_comm_bw += comm_bw
                
                if num_batches % 10 == 0:
                    print(f"Rank {rank} batch {num_batches} å†…è®­ç»ƒä¸­, loss: {loss.item():.4f}, acc: {acc:.4f}")
                iter += 1
        
        # ====== é€šä¿¡ç»Ÿè®¡ ======
        epoch_comm = comm_monitor.end_time - comm_monitor.start_time
        epoch_comm_kb, epoch_comm_bw = comm_monitor.get_bandwidth()
        print(f"Rank {rank} é€šä¿¡é‡: {epoch_comm_kb:.4f} KB, {epoch_comm_bw:.4f} KB/s")
        # 7ï¸âƒ£ åŒæ­¥å…¶ä»–è¿›ç¨‹çš„æ‰¹æ¬¡è®­ç»ƒï¼ˆç¡®ä¿åˆ†å¸ƒå¼è®­ç»ƒåŒæ­¥ï¼‰
        if iter > local_num_batches:
            for _ in range(max_num_batches - local_num_batches):
                dist.all_reduce(torch.zeros(1, device=device))

        # ======================================================
        # ğŸ“Š ç²¾åº¦ä¸è€—æ—¶ç»Ÿè®¡
        # ======================================================
        # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®åº¦
        avg_loss = total_loss / num_batches
        avg_acc = total_acc / num_batches   
        # åŒæ­¥æ‰€æœ‰è¿›ç¨‹çš„æŸå¤±å’Œå‡†ç¡®åº¦
        avg_loss = torch.tensor(avg_loss, device=device)
        avg_acc = torch.tensor(avg_acc, device=device)
        dist.all_reduce(avg_loss, op=dist.ReduceOp.AVG)
        dist.all_reduce(avg_acc, op=dist.ReduceOp.AVG)
        test_acc = evaluate(model, test_loader, device, rank, world_size, subg)
        # ======================================================
        # ğŸ“ˆ TensorBoard å†™å…¥
        # ======================================================
        if rank == 0:
            writer.add_scalar("Loss/train", avg_loss.item(), epoch)
            writer.add_scalar("Accuracy/train", avg_acc.item(), epoch)
            writer.add_scalar("Accuracy/test", test_acc, epoch)
            writer.add_scalar("Time/forward", epoch_forward / num_batches, epoch)
            writer.add_scalar("Time/backward", epoch_backward / num_batches, epoch)
            writer.add_scalar("Time/comm", epoch_comm / num_batches, epoch)
            writer.add_scalar("Time/batch", epoch_batch / num_batches, epoch)
            writer.add_scalar("Comm/volume_KB", epoch_comm_kb / num_batches, epoch)
            writer.add_scalar("Comm/bandwidth_KBps", epoch_comm_bw / num_batches, epoch)
            writer.add_scalar("GPU/memory_allocated_MB", gpu_memory_allocated, epoch)
            writer.add_scalar("GPU/memory_reserved_MB", gpu_memory_reserved, epoch)
            writer.add_scalar("GPU/utilization_percent", gpu_utilization, epoch)

        if rank == 0:
                print(f"RANK:{rank} - [Epoch {epoch+1}] Loss={avg_loss.item():.4f}, TrainAcc={avg_acc.item():.4f}, TestAcc={test_acc:.4f}")

    # å…³é—­ TensorBoard writer
    if rank == 0:
        writer.close()
        print(f"Rank {rank} training completed.")


# ==========================================================
# ğŸ” è¯„ä¼°å‡½æ•°
# ==========================================================
def evaluate(model, test_loader, device, rank, world_size, subg):
    """
    æ¨¡å‹è¯„ä¼°å‡½æ•°
    
    å…³é”®å‚æ•°è®¾ç½®:
    - model: å¾…è¯„ä¼°çš„GNNæ¨¡å‹
    - test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
    - device: è®¡ç®—è®¾å¤‡
    - rank: å½“å‰è¿›ç¨‹æ’å
    - world_size: è¿›ç¨‹æ€»æ•°
    """
    model.eval()
    total_accuracy = 0.0
    num_batches = 0
    
    with torch.no_grad():
        for input_nodes, pos_pair_graph, neg_pair_graph, blocks in test_loader:
            # ============ æ•°æ®å‡†å¤‡ ============
            feats = subg.ndata['feat'][input_nodes].to(device)
            
            # ============ å‰å‘ä¼ æ’­ ============
            node_emb = model(blocks, feats)
            
            # ============ æ­£æ ·æœ¬å¾—åˆ†è®¡ç®— ============
            pos_src, pos_dst = pos_pair_graph.edges()
            pos_src_emb = node_emb[pos_src]
            pos_dst_emb = node_emb[pos_dst]
            pos_score = (pos_src_emb * pos_dst_emb).sum(dim=1)
            
            # ============ è´Ÿæ ·æœ¬å¾—åˆ†è®¡ç®— ============
            neg_src, neg_dst = neg_pair_graph.edges()
            neg_src_emb = node_emb[neg_src]
            neg_dst_emb = node_emb[neg_dst]
            neg_score = (neg_src_emb * neg_dst_emb).sum(dim=1)
            
            # ============ æŸå¤±è®¡ç®— ============
            scores = torch.cat([pos_score, neg_score], dim=0)
            labels = torch.cat([
                torch.ones_like(pos_score),
                torch.zeros_like(neg_score)
            ]).to(device)
            
            # ============ å‡†ç¡®åº¦è®¡ç®— ============
            predictions = (torch.sigmoid(scores) > 0.5).float()
            accuracy = (predictions == labels).float().mean().item()
            
            total_accuracy += accuracy
            num_batches += 1
    
    # ============ åˆ†å¸ƒå¼åŒæ­¥ ============
    avg_accuracy = total_accuracy / num_batches if num_batches > 0 else 0.0
    avg_accuracy_tensor = torch.tensor(avg_accuracy, device=device)
    dist.all_reduce(avg_accuracy_tensor, op=dist.ReduceOp.SUM)
    avg_accuracy_tensor /= world_size
    
    model.train()
    return avg_accuracy_tensor.item()


# ==========================================================
# 6ï¸âƒ£ ä¸»å…¥å£
# ==========================================================
if __name__ == "__main__":
    """
    ä¸»ç¨‹åºå…¥å£
    
    å…³é”®å‚æ•°è®¾ç½®:
    - graph_dir: å›¾æ•°æ®å­˜å‚¨ç›®å½•
    - num_parts: å›¾åˆ’åˆ†ä»½æ•°ï¼Œå¯¹åº”åˆ†å¸ƒå¼è¿›ç¨‹æ•°
    - nodes: å›¾èŠ‚ç‚¹æ•°é‡
    - world_size: åˆ†å¸ƒå¼è¿›ç¨‹æ•°é‡
    """
    # å…³é”®å‚æ•°: å›¾æ•°æ®é…ç½®
    graph_dir = prepare_graph(graph_dir="datasets/link_prediction_ER", num_parts=3, nodes=1000)
    world_size = 3
    device = None
    
    # ä½¿ç”¨mp.spawnå¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
    mp.spawn(
        train_fn,
        args=(world_size, graph_dir),
        nprocs=world_size,
        join=True
    )
