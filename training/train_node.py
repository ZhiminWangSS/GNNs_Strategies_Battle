import os
import time
import torch
import torch.nn as nn
import dgl
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.tensorboard import SummaryWriter
import sys
import psutil
import GPUtil
import torch.multiprocessing as mp

# ==========================================================
# æ·»åŠ å½“å‰ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
# ==========================================================
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from models.gcn import GCN
from datasets.data_generator import GraphGenerator
import datetime


# ==========================================================
# 1ï¸âƒ£ åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ
# ==========================================================
def setup_distributed(rank, world_size):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
    os.environ["MASTER_ADDR"] = "127.0.0.1"
    os.environ["MASTER_PORT"] = "29500"
    os.environ["RANK"] = str(rank)
    os.environ["WORLD_SIZE"] = str(world_size)
    
    torch.cuda.set_device(rank)
    dist.init_process_group(backend="nccl", 
                            rank=rank, 
                            world_size=world_size)
    device = torch.device(f"cuda:{rank}")
    
    print(f"ğŸš€ Rank {rank}: åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ")
    return device


def train_fn(rank, world_size, graph_dir, num_epochs=10, lr=0.001):
    """åˆ†å¸ƒå¼è®­ç»ƒå‡½æ•°ï¼Œç”±mp.spawnè°ƒç”¨"""
    device = None
    try:
        device = setup_distributed(rank, world_size)
        local_rank = rank
        print(f"Rank {rank} å¯åŠ¨è®­ç»ƒè¿›ç¨‹")
        # è®­ç»ƒå‡½æ•°è°ƒç”¨
        train(rank, local_rank, world_size, device, graph_dir=graph_dir, num_epochs=num_epochs, lr=lr)
    except Exception as e:
        print(f"Rank {rank} è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
    finally:
        # åªåœ¨è¿›ç¨‹ç»„æˆåŠŸåˆå§‹åŒ–åæ‰é”€æ¯
        if dist.is_initialized():
            dist.destroy_process_group()


# ==========================================================
# 2ï¸âƒ£ å›¾ç”Ÿæˆä¸åˆ’åˆ†
# ==========================================================
def prepare_graph(graph_dir="datasets/graph_parts", num_parts=3, nodes=20):
    """ç”Ÿæˆæˆ–åŠ è½½åˆ’åˆ†å¥½çš„å›¾"""
    if not os.path.exists(graph_dir):
        print(f"ğŸ”§ ç”Ÿæˆæ–°çš„å›¾æ•°æ®å¹¶åˆ’åˆ†...")
        os.makedirs(graph_dir, exist_ok=True)
        gen = GraphGenerator()
        G_nx = gen.generate_nx_graph(kind='ER', n_nodes=nodes, p=0.01,)
        g = gen.nx_to_dgl(G_nx)
        gen.add_node_labels(g)

        # åŒæ—¶ç”Ÿæˆ metis å’Œ random åˆ’åˆ†
        gen.partition_graph(g, num_parts=num_parts, method='metis', output_dir=graph_dir)

        print(f"âœ… å›¾æ•°æ®ç”Ÿæˆå’Œåˆ’åˆ†å®Œæˆ")

    return graph_dir


# ==========================================================
# 3ï¸âƒ£ è®­ç»ƒå‡½æ•°
# ==========================================================
def train(rank, local_rank, world_size, device, graph_dir, num_epochs=20, lr=0.001, partition_method="metis"):
    
    torch.manual_seed(0)

    # åˆå§‹åŒ– TensorBoardï¼Œä»… rank 0 å†™æ—¥å¿—
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    writer = SummaryWriter(log_dir=f"runs/node_classification_{timestamp}_rank{rank}") if rank == 0 else None

    # æ•°æ®åŠ è½½å™¨åˆå§‹åŒ– - æ¯ä¸ªrankåŠ è½½å¯¹åº”çš„å­å›¾åˆ†åŒº
    gen = GraphGenerator()
    train_loader, test_loader, subg = gen.get_dataloader_for_node_classification(
        pid=rank,
        partition_method=partition_method,
        batch_size=32,
        train_ratio=0.8,
        num_workers=0,
        device=device,
        sampler_fanouts=[10, 5],
        partition_dir=graph_dir
    )

    if rank == 0:
        print(f"ğŸ“Š Rank {rank} åŠ è½½å®Œæˆ dataloaderï¼ˆå­å›¾ {rank}ï¼‰")

    # æ¨¡å‹åˆå§‹åŒ–
    input_dim = 4
    hidden_dim = 64
    num_classes = subg.ndata['labels'].max().item() + 1
    gnn = GCN(input_dim, hidden_dim, num_classes, dropout=0.0).to(device)
    gnn = DDP(gnn, device_ids=[local_rank], output_device=local_rank)
    optimizer = torch.optim.Adam(gnn.parameters(), lr=lr)

    if rank == 0:
        print(f"ğŸš€ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ: {sum(p.numel() for p in gnn.parameters()):,} å‚æ•°")

    # ============ å¼€å§‹è®­ç»ƒ ============ 
    gnn.train()
    for epoch in range(num_epochs):
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0

        for input_nodes, output_nodes, blocks in train_loader:
            # 1ï¸âƒ£ è·å–èŠ‚ç‚¹ç‰¹å¾å’Œæ ‡ç­¾
            feats = blocks[0].srcdata["feat"].to(device)
            labels = blocks[-1].dstdata["labels"].to(device)  # æœ€åä¸€å±‚ block çš„ dst èŠ‚ç‚¹æ ‡ç­¾
            
            # è°ƒè¯•ï¼šæ£€æŸ¥ç»´åº¦
            print(f"Debug - input_nodes length: {len(input_nodes)}")
            print(f"Debug - output_nodes length: {len(output_nodes)}")
            print(f"Debug - blocks[0] src nodes: {blocks[0].num_src_nodes()}, dst nodes: {blocks[0].num_dst_nodes()}")
            print(f"Debug - blocks[-1] src nodes: {blocks[-1].num_src_nodes()}, dst nodes: {blocks[-1].num_dst_nodes()}")
            print(f"Debug - feats shape: {feats.shape}")
            print(f"Debug - labels shape: {labels.shape}")

            # 2ï¸âƒ£ å‰å‘ä¼ æ’­
            logits = gnn(blocks, feats)

            print(f"Debug - logits shape: {logits.shape}")

            # 3ï¸âƒ£ è®¡ç®—äº¤å‰ç†µæŸå¤±
            loss = nn.functional.cross_entropy(logits, labels)

            # 4ï¸âƒ£ åå‘ä¼ æ’­ä¸ä¼˜åŒ–
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # 5ï¸âƒ£ è®¡ç®—å‡†ç¡®ç‡
            preds = logits.argmax(dim=1)
            acc = (preds == labels).float().mean().item()

            total_loss += loss.item()
            total_accuracy += acc
            num_batches += 1

            if num_batches % 10 == 0:
                print(f"Rank {rank} batch {num_batches}, loss: {loss.item():.4f}, acc: {acc:.4f}")

        # åŒæ­¥å¹³å‡ loss å’Œ accuracy
        avg_loss_tensor = torch.tensor(total_loss / max(num_batches, 1), device=device)
        avg_acc_tensor = torch.tensor(total_accuracy / max(num_batches, 1), device=device)
        dist.all_reduce(avg_loss_tensor, op=dist.ReduceOp.AVG)
        dist.all_reduce(avg_acc_tensor, op=dist.ReduceOp.AVG)
        avg_loss = avg_loss_tensor.item()
        avg_accuracy = avg_acc_tensor.item()

        if writer and rank == 0:
            writer.add_scalar("Loss/train", avg_loss, epoch)
            writer.add_scalar("Accuracy/train", avg_accuracy, epoch)

        print(f"Rank {rank} [Epoch {epoch+1}/{num_epochs}] å¹³å‡æŸå¤±: {avg_loss:.4f}, å‡†ç¡®åº¦: {avg_accuracy:.4f}")

    dist.destroy_process_group()
    if writer:
        writer.close()
    if rank == 0:
        print("âœ… Node classification è®­ç»ƒå®Œæˆï¼")



# ==========================================================
# 4ï¸âƒ£ ä¸»å…¥å£
# ==========================================================
if __name__ == "__main__":
    graph_dir = prepare_graph(graph_dir="datasets/node_cls_small_twolayer", num_parts=3, nodes=200)
    world_size = 3
    device = None
    # ä½¿ç”¨mp.spawnå¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
    mp.spawn(
        train_fn,
        args=(world_size, graph_dir),
        nprocs=world_size,
        join=True
    )
