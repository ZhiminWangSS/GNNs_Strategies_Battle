import os
import time
import torch
import torch.nn as nn
import dgl
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.tensorboard import SummaryWriter
import sys
import psutil
import GPUtil
import torch.multiprocessing as mp
import datetime

# ==========================================================
# æ·»åŠ è·¯å¾„
# ==========================================================
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from models.gcn import GCN
from datasets.data_generator import GraphGenerator


# ==========================================================
# 1ï¸âƒ£ åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
# ==========================================================
def setup_distributed(rank, world_size):
    os.environ["MASTER_ADDR"] = "127.0.0.1"
    os.environ["MASTER_PORT"] = "29500"
    torch.cuda.set_device(rank)
    dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)
    device = torch.device(f"cuda:{rank}")
    print(f"ğŸš€ Rank {rank}: åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ")
    return device


# ==========================================================
# 2ï¸âƒ£ é€šä¿¡ç»Ÿè®¡æ¨¡å—ï¼ˆhookï¼‰
# ==========================================================
class CommunicationMonitor:
    def __init__(self):
        self.total_bytes = 0
        self.start_time = None
        self.end_time = None

    def start(self):
        self.total_bytes = 0
        self.start_time = time.time()

    def hook(self, tensor):
        """æ¯æ¬¡é€šä¿¡è‡ªåŠ¨ç»Ÿè®¡æ•°æ®é‡"""
        self.total_bytes += tensor.numel() * tensor.element_size()
        return tensor

    def stop(self):
        self.end_time = time.time()

    def get_bandwidth(self):
        duration = max(1e-6, self.end_time - self.start_time)
        mb = self.total_bytes / (1024 * 1024)
        return mb, mb / duration  # (é€šä¿¡é‡ MB, å¸¦å®½ MB/s)


# ==========================================================
# 3ï¸âƒ£ è¯„ä¼°å‡½æ•°
# ==========================================================
def evaluate(model, test_loader, device, rank, world_size, subg):
    model.eval()
    total_acc = 0.0
    num_batches = 0

    with torch.no_grad():
        for input_nodes, output_nodes, blocks in test_loader:
            feats = subg.ndata["feat"][input_nodes].to(device)
            labels = subg.ndata["labels"][output_nodes].to(device)
            logits = model(blocks, feats)
            preds = logits.argmax(dim=1)
            acc = (preds == labels).float().mean().item()
            total_acc += acc
            num_batches += 1

    avg_acc = total_acc / num_batches if num_batches > 0 else 0
    acc_tensor = torch.tensor(avg_acc, device=device)
    dist.all_reduce(acc_tensor, op=dist.ReduceOp.AVG)
    return acc_tensor.item()


# ==========================================================
# 4ï¸âƒ£ ä¸»è®­ç»ƒå‡½æ•°
# ==========================================================
def train(rank, local_rank, world_size, device, graph_dir, num_epochs=20, lr=0.001, partition_method="metis"):
    torch.manual_seed(0)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    writer = SummaryWriter(log_dir=f"runs/node_cls_rank{rank}_{timestamp}") if rank == 0 else None

    # æ•°æ®åŠ è½½
    gen = GraphGenerator()
    train_loader, test_loader, subg = gen.get_dataloader_for_node_classification(
        pid=rank,
        partition_method=partition_method,
        batch_size=32,
        train_ratio=0.8,
        device=device,
        sampler_fanouts=[10, 5],
        partition_dir=graph_dir,
    )
    subg = subg.to(device)

    # æ¨¡å‹åˆå§‹åŒ–
    input_dim = 4
    hidden_dim = 64
    num_classes = subg.ndata["labels"].max().item() + 1
    model = GCN(input_dim, hidden_dim, num_classes, dropout=0.0).to(device)
    model = DDP(model, device_ids=[local_rank], output_device=local_rank)

    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    comm_monitor = CommunicationMonitor()

    if rank == 0:
        print(f"ğŸš€ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ: {sum(p.numel() for p in model.parameters()):,} å‚æ•°")

    # ======================================================
    # ğŸ” è®­ç»ƒå¾ªç¯
    # ======================================================
    for epoch in range(num_epochs):

            model.train()
            total_loss, total_acc = 0.0, 0.0
            num_batches = 0
            epoch_forward, epoch_backward, epoch_comm, epoch_batch = 0.0, 0.0, 0.0, 0.0
            epoch_comm_mb, epoch_comm_bw = 0.0, 0.0
            comm_monitor.start()

            for input_nodes, output_nodes, blocks in train_loader:
                batch_start = time.time()

                # ====== 1ï¸âƒ£ æ•°æ®å‡†å¤‡ ======
                feats = subg.ndata["feat"][input_nodes].to(device)
                labels = subg.ndata["labels"][output_nodes].to(device)

                # ====== 2ï¸âƒ£ å‰å‘ä¼ æ’­ ======
                t0 = time.time()
                logits = model(blocks, feats)
                forward_time = time.time() - t0

                # ====== 3ï¸âƒ£ æŸå¤±ä¸åå‘ä¼ æ’­ ======
                loss = nn.functional.cross_entropy(logits, labels)
                t1 = time.time()
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                backward_time = time.time() - t1

                # ====== 4ï¸âƒ£ é€šä¿¡ç»Ÿè®¡ ======
                comm_monitor.stop()
                comm_mb, comm_bw = comm_monitor.get_bandwidth()

                # ====== 5ï¸âƒ£ ç²¾åº¦ä¸è€—æ—¶ç»Ÿè®¡ ======
                preds = logits.argmax(dim=1)
                acc = (preds == labels).float().mean().item()
                batch_time = time.time() - batch_start

                # ====== GPUå†…å­˜ç›‘æ§ ======
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                    gpu_memory_allocated = torch.cuda.memory_allocated(device) / (1024**2)  # MB
                    gpu_memory_reserved = torch.cuda.memory_reserved(device) / (1024**2)  # MB
                    gpu_utilization = GPUtil.getGPUs()[rank].load * 100  # %
                else:
                    gpu_memory_allocated = 0.0
                    gpu_memory_reserved = 0.0
                    gpu_utilization = 0.0

                total_loss += loss.item()
                total_acc += acc
                num_batches += 1
                epoch_forward += forward_time
                epoch_backward += backward_time
                epoch_comm += comm_monitor.end_time - comm_monitor.start_time
                epoch_batch += batch_time
                epoch_comm_bw += comm_bw
                epoch_comm_mb += comm_mb
                if num_batches % 10 == 0 and rank == 0:
                    print(f"[Rank {rank}] Batch {num_batches}: Loss={loss.item():.4f}, Acc={acc:.4f}, BW={comm_bw:.2f}MB/s, GPU Mem={gpu_memory_allocated:.2f}GB, GPU Util={gpu_utilization:.1f}%")



            # ===== åŒæ­¥å¹³å‡ =====
            avg_loss = torch.tensor(total_loss / max(1, num_batches), device=device)
            avg_acc = torch.tensor(total_acc / max(1, num_batches), device=device)
            dist.all_reduce(avg_loss, op=dist.ReduceOp.AVG)
            dist.all_reduce(avg_acc, op=dist.ReduceOp.AVG)

            # if epoch % 10 == 0:
            # ===== è¯„ä¼° =====
            test_acc = evaluate(model, test_loader, device, rank, world_size, subg)
            # ===== å†™å…¥TensorBoard =====
            if rank == 0 and writer:
                writer.add_scalar("Loss/train", avg_loss.item(), epoch)
                writer.add_scalar("Accuracy/train", avg_acc.item(), epoch)
                writer.add_scalar("Accuracy/test", test_acc, epoch)
                writer.add_scalar("Time/forward", epoch_forward / num_batches, epoch)
                writer.add_scalar("Time/backward", epoch_backward / num_batches, epoch)
                writer.add_scalar("Time/comm", epoch_comm / num_batches, epoch)
                writer.add_scalar("Time/batch", epoch_batch / num_batches, epoch)
                writer.add_scalar("Comm/volume_MB", epoch_comm_mb / num_batches, epoch)
                writer.add_scalar("Comm/bandwidth_MBps", epoch_comm_bw / num_batches, epoch)
                writer.add_scalar("GPU/memory_allocated_GB", gpu_memory_allocated, epoch)
                writer.add_scalar("GPU/memory_reserved_GB", gpu_memory_reserved, epoch)
                writer.add_scalar("GPU/utilization_percent", gpu_utilization, epoch)

            if rank == 0:
                print(f"RANK:{rank} - [Epoch {epoch+1}] Loss={avg_loss.item():.4f}, TrainAcc={avg_acc.item():.4f}, TestAcc={test_acc:.4f}")

    if writer:
        writer.close()
    dist.barrier()
    if rank == 0:
        print("âœ… è®­ç»ƒå®Œæˆï¼Œæ‰€æœ‰æŒ‡æ ‡å·²å†™å…¥ TensorBoardã€‚")


# ==========================================================
# 5ï¸âƒ£ åˆ†å¸ƒå¼å…¥å£
# ==========================================================
def train_fn(rank, world_size, graph_dir, num_epochs=50, lr=0.001):
    device = setup_distributed(rank, world_size)
    train(rank, rank, world_size, device, graph_dir, num_epochs=num_epochs, lr=lr)
    dist.destroy_process_group()


# ==========================================================
# 6ï¸âƒ£ å›¾å‡†å¤‡ + å¯åŠ¨
# ==========================================================
def prepare_graph(graph_dir="datasets/node_cls", num_parts=3, nodes=1000):
    if not os.path.exists(graph_dir):
        gen = GraphGenerator()
        G_nx = gen.generate_nx_graph(kind="ER", n_nodes=nodes, p=0.01)
        g = gen.nx_to_dgl(G_nx)
        gen.add_node_labels(g)
        gen.partition_graph(g, num_parts=num_parts, method="metis", output_dir=graph_dir)
    return graph_dir


if __name__ == "__main__":
    world_size = 3
    graph_dir = prepare_graph(graph_dir="datasets/node_cls_ER", num_parts=3, nodes=1000)
    num_epochs = 50
    mp.spawn(train_fn, args=(world_size, graph_dir), nprocs=world_size, join=True)
