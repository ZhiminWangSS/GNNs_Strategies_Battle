import os
import time
import torch
from torch.utils.tensorboard import SummaryWriter
from models.gcn import GCN
from utils.graph_partition import partition_dgl_graph

import dgl
import dgl.nn.pytorch as dglnn
from dgl.dataloading import EdgeDataLoader, NeighborSampler, as_edge_prediction_sampler
from dgl.distributed import DistGraph, partition_graph, initialize, finalize


graph_name = "synthetic_lp_graph"
graph_dir = "graph_partitions"
graph_bin = "synthetic_lp_graph.bin"
edge_file = "link_prediction_edges.pt"
num_parts = 4
partition_method = "metis"  # å¯é€‰å€¼ï¼š'metis' / 'random' / 'non_uniform'
num_epochs = 5
batch_size = 1024
lr = 1e-3
device = "cuda" if torch.cuda.is_available() else "cpu"


# ==========================================================
# 2ï¸âƒ£ è¯»å–å›¾æ•°æ®
# ==========================================================
print("ğŸ“¦ åŠ è½½å›¾æ•°æ®ä¸­ ...")
g_list, _ = dgl.load_graphs(graph_bin)
g = g_list[0]

# åŠ è½½è¾¹æ ·æœ¬ï¼ˆæ­£è´Ÿæ ·æœ¬ï¼‰
edge_data = torch.load(edge_file)
edges = edge_data["edges"]
labels = edge_data["labels"]

print(f"å›¾èŠ‚ç‚¹æ•°: {g.num_nodes()} | è¾¹æ•°: {g.num_edges()}")
print(f"é“¾è·¯é¢„æµ‹æ ·æœ¬æ•°é‡: {len(edges)}")



# ==========================================================
# 3ï¸âƒ£ å›¾åˆ’åˆ†ï¼ˆæ ¹æ®é€‰æ‹©ï¼‰
# ==========================================================
os.makedirs(graph_dir, exist_ok=True)

partition_dgl_graph(g, graph_name, graph_dir, partition_method, num_parts)

# ä»…åœ¨åˆ’åˆ†æ–‡ä»¶ä¸å­˜åœ¨æ—¶æ‰§è¡Œåˆ’åˆ†
if not os.path.exists(os.path.join(graph_dir, graph_name + ".json")):
    partition_dgl_graph(g, graph_name, graph_dir, partition_method, num_parts)


# ==========================================================
# 4ï¸âƒ£ åˆå§‹åŒ–åˆ†å¸ƒå¼å›¾ï¼ˆå•æœºå¤šå¡ç¯å¢ƒï¼‰
# ==========================================================
print("ğŸš€ åˆå§‹åŒ–åˆ†å¸ƒå¼å›¾ ...")
initialize("graph_partitions")  # åˆå§‹åŒ– DGL åˆ†å¸ƒå¼å›¾å¼•æ“
dist_g = DistGraph(graph_name, part_config=os.path.join(graph_dir, graph_name + ".json"))
print("åˆ†å¸ƒå¼å›¾åŠ è½½æˆåŠŸ âœ…")


# ==========================================================
# 5ï¸âƒ£ æ„å»ºé‡‡æ ·å™¨ä¸æ•°æ®åŠ è½½å™¨
# ==========================================================
# é‡‡ç”¨é‚»å±…é‡‡æ ·ç­–ç•¥ (2-hop)
sampler = as_edge_prediction_sampler(
    NeighborSampler([10, 10]),
    negative_sampler=dgl.dataloading.negative_sampler.Uniform(1)
)

# è¿™é‡Œæˆ‘ä»¬ç®€å•ä½¿ç”¨æ‰€æœ‰è¾¹ç´¢å¼•
edge_ids = torch.arange(g.num_edges())

# EdgeDataLoader æ”¯æŒå¤šè¿›ç¨‹åˆ†å¸ƒå¼åŠ è½½
dataloader = EdgeDataLoader(
    dist_g,
    edge_ids,
    sampler,
    batch_size=batch_size,
    shuffle=True,
    num_workers=0
)

# ==========================================================
# 6ï¸âƒ£ åˆå§‹åŒ– TensorBoard è®°å½•å™¨
# ==========================================================
writer = SummaryWriter(log_dir='runs/link_prediction_experiment')

# ==========================================================
# 7ï¸âƒ£ åˆå§‹åŒ– GNN æ¨¡å‹ï¼ˆé“¾è·¯é¢„æµ‹ä»»åŠ¡ï¼‰
# ==========================================================
# åŸºäº OGBL-ddi æ•°æ®é›†ï¼šè¾“å…¥ç»´åº¦128ï¼Œè¾“å‡ºç»´åº¦1ï¼ˆè¾¹å­˜åœ¨æ¦‚ç‡ï¼‰
input_dim = 128  # èŠ‚ç‚¹ç‰¹å¾ç»´åº¦
hidden_dim = 64  # éšè—å±‚ç»´åº¦
output_dim = 1   # é“¾è·¯é¢„æµ‹è¾“å‡ºç»´åº¦ï¼ˆè¾¹å­˜åœ¨æ¦‚ç‡ï¼‰

gnn = GCN(input_dim, hidden_dim, output_dim, dropout=0.5)
gnn = gnn.to(device)

# ä¼˜åŒ–å™¨
optimizer = torch.optim.Adam(gnn.parameters(), lr=lr)

print(f"ğŸš€ GNN æ¨¡å‹åˆå§‹åŒ–å®Œæˆ:")
print(f"   è¾“å…¥ç»´åº¦: {input_dim}")
print(f"   éšè—ç»´åº¦: {hidden_dim}")
print(f"   è¾“å‡ºç»´åº¦: {output_dim}")
print(f"   æ€»å‚æ•°é‡: {sum(p.numel() for p in gnn.parameters()):,}")


# ==========================================================
# 8ï¸âƒ£ è®­ç»ƒå¾ªç¯
# ==========================================================
print("ğŸ¯ å¼€å§‹è®­ç»ƒé“¾è·¯é¢„æµ‹æ¨¡å‹ ...")

# åˆå§‹åŒ–å¸¦å®½ç›‘æ§å˜é‡
prev_comm_time = 0

gnn.train()
for epoch in range(num_epochs):
    total_loss = 0
    num_batches = 0
    epoch_comm_time = 0
    epoch_forward_time = 0
    epoch_backward_time = 0
    
    for input_nodes, pair_graph, blocks in dataloader:
        # é€šä¿¡æ—¶é—´ç›‘æ§å¼€å§‹
        comm_start_time = time.time()
        
        # è·å–èŠ‚ç‚¹ç‰¹å¾
        input_features = dist_g.ndata['feat'][input_nodes].to(device)
        
        # é€šä¿¡æ—¶é—´ç›‘æ§ç»“æŸ
        comm_end_time = time.time()
        comm_time = comm_end_time - comm_start_time
        epoch_comm_time += comm_time
        
        # å‰å‘ä¼ æ’­æ—¶é—´ç›‘æ§
        forward_start_time = time.time()
        node_embeddings = gnn(blocks[0], input_features)
        forward_end_time = time.time()
        forward_time = forward_end_time - forward_start_time
        epoch_forward_time += forward_time
        
        # è·å–è¾¹é¢„æµ‹ç»“æœ
        src_embeddings = node_embeddings[pair_graph.edges()[0]]
        dst_embeddings = node_embeddings[pair_graph.edges()[1]]
        
        # è®¡ç®—è¾¹å¾—åˆ†ï¼ˆç‚¹ç§¯ç›¸ä¼¼åº¦ï¼‰
        edge_scores = torch.sum(src_embeddings * dst_embeddings, dim=1)
        
        # è·å–æ ‡ç­¾
        edge_labels = pair_graph.edata['label'].to(device).float()
        
        # è®¡ç®—æŸå¤±ï¼ˆäºŒå…ƒäº¤å‰ç†µï¼‰
        loss = torch.nn.functional.binary_cross_entropy_with_logits(
            edge_scores, edge_labels
        )
        
        # åå‘ä¼ æ’­æ—¶é—´ç›‘æ§
        backward_start_time = time.time()
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        backward_end_time = time.time()
        backward_time = backward_end_time - backward_start_time
        epoch_backward_time += backward_time
        
        total_loss += loss.item()
        num_batches += 1
    
    avg_loss = total_loss / num_batches if num_batches > 0 else 0
    avg_comm_time = epoch_comm_time / num_batches if num_batches > 0 else 0
    avg_forward_time = epoch_forward_time / num_batches if num_batches > 0 else 0
    avg_backward_time = epoch_backward_time / num_batches if num_batches > 0 else 0
    
    # è®¡ç®—å¸¦å®½æ³¢åŠ¨ï¼ˆåŸºäºé€šä¿¡æ—¶é—´çš„å˜åŒ–ï¼‰
    if epoch > 0:
        bandwidth_variation = abs(avg_comm_time - prev_comm_time) / prev_comm_time if prev_comm_time > 0 else 0
        writer.add_scalar('Bandwidth/variation', bandwidth_variation, epoch)
    prev_comm_time = avg_comm_time
    
    # è®°å½•åˆ° TensorBoard
    writer.add_scalar('Loss/train', avg_loss, epoch)
    writer.add_scalar('Time/communication', avg_comm_time, epoch)
    writer.add_scalar('Time/forward', avg_forward_time, epoch)
    writer.add_scalar('Time/backward', avg_backward_time, epoch)
    writer.add_scalar('Time/total', avg_comm_time + avg_forward_time + avg_backward_time, epoch)
    
    print(f"ğŸ“Š Epoch [{epoch+1}/{num_epochs}] | å¹³å‡æŸå¤±: {avg_loss:.4f}")
    print(f"   â±ï¸  é€šä¿¡æ—¶é—´: {avg_comm_time:.4f}s | å‰å‘æ—¶é—´: {avg_forward_time:.4f}s | åå‘æ—¶é—´: {avg_backward_time:.4f}s")


# ==========================================================
# 9ï¸âƒ£ æ¸…ç†èµ„æº
# ==========================================================
print("ğŸ§¹ æ¸…ç†åˆ†å¸ƒå¼èµ„æº ...")

# å…³é—­ TensorBoard è®°å½•å™¨
writer.close()

finalize()
print("âœ… è®­ç»ƒå®Œæˆï¼")
print("ğŸ“ˆ è®­ç»ƒæ—¥å¿—å·²ä¿å­˜åˆ° 'runs/link_prediction_experiment'ï¼Œä½¿ç”¨ 'tensorboard --logdir=runs' æŸ¥çœ‹")
