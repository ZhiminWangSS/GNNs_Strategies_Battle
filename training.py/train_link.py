import os
import time
import torch
import torch.nn as nn
import dgl
import dgl.nn.pytorch as dglnn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel
from torch.utils.tensorboard import SummaryWriter
import sys

# æ·»åŠ å½“å‰ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„ï¼Œç¡®ä¿å¯ä»¥æ­£ç¡®å¯¼å…¥modelsæ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
# from dgl.dataloading import EdgeDataLoader, as_edge_prediction_sampler, NeighborSampler

from models.gcn import GCN
from datasets.data_generator import GraphGenerator

# ==========================================================
# 1ï¸âƒ£ è®¾ç½®è®­ç»ƒå‚æ•°
# ==========================================================
# åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°


os.environ["RANK"] = "0"
os.environ["WORLD_SIZE"] = "4"
os.environ["LOCAL_RANK"] = "0"
os.environ["MASTER_ADDR"] = "127.0.0.1"
os.environ["MASTER_PORT"] = "29500"
local_rank = int(os.environ.get("LOCAL_RANK", 0))
world_size = int(os.environ.get("WORLD_SIZE", 4))
rank = int(os.environ.get("RANK", 0))

# è®­ç»ƒå‚æ•°
num_epochs = 100
lr = 0.001
batch_size = 1024
num_workers = 0

# å›¾åˆ’åˆ†å‚æ•°
graph_dir = "datasets/graph_parts"
num_parts = 3

# è®¾å¤‡è®¾ç½®
torch.cuda.set_device(local_rank)
device = torch.device(f"cuda:{local_rank}")

# ==========================================================
# 2ï¸âƒ£ åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ
# ==========================================================
print(f"ğŸš€ åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ...")
print(f"   Rank: {rank} | Local Rank: {local_rank} | World Size: {world_size}")
# å¦‚æœç¯å¢ƒå˜é‡RANKæœªå®šä¹‰ï¼Œåˆ™é»˜è®¤è®¾ä¸º0

# dist.init_process_group(backend="nccl")

# ==========================================================
# 3ï¸âƒ£ ç”Ÿæˆå›¾æ•°æ®å¹¶åˆ’åˆ†
# ==========================================================
print(f"ğŸ“Š ç”Ÿæˆå›¾æ•°æ®å¹¶åˆ’åˆ†...")

# æ£€æŸ¥å›¾åˆ’åˆ†æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ç”Ÿæˆ
if not os.path.exists(graph_dir):
    print(f"ğŸ”§ ç”Ÿæˆæ–°çš„å›¾æ•°æ®å¹¶åˆ’åˆ†...")
    os.makedirs(graph_dir, exist_ok=True)
    
    # ç”Ÿæˆå›¾æ•°æ®
    gen = GraphGenerator()
    G_nx = gen.generate_nx_graph(kind='ER', n_nodes=2000, p=0.01)
    g = gen.nx_to_dgl(G_nx)
    gen.add_node_labels(g)
    
    # åˆ’åˆ†ä¸ºå­å›¾
    gen.partition_graph_for_node_classification(g, num_parts=num_parts, method='metis', output_dir=graph_dir)
    gen.partition_graph_for_node_classification(g, num_parts=num_parts, method='random', output_dir=graph_dir)
    
    print(f"âœ… å›¾æ•°æ®ç”Ÿæˆå’Œåˆ’åˆ†å®Œæˆ")

# åŠ è½½åˆ†åŒºå›¾
part_config = os.path.join(graph_dir, "synthetic_lp_graph.json")
if not os.path.exists(part_config):
    print(f"âŒ å›¾åˆ’åˆ†é…ç½®æ–‡ä»¶ {part_config} ä¸å­˜åœ¨")
    exit(1)



# # åŠ è½½åˆ†åŒºå›¾
# dist_g = dgl.distributed.DistGraph(
#     graph_name="synthetic_lp_graph",
#     part_config=part_config
# )
gen = GraphGenerator()
loader, _ = gen.get_dataloader_for_node_classification(pid=0, partition_method='metis', partition_dir=graph_dir)

# print(f"âœ… å›¾æ•°æ®åŠ è½½å®Œæˆ:")
# print(f"   å›¾åç§°: {dist_g.graph_name}")
# print(f"   åˆ†åŒºæ•°é‡: {num_parts}")
# print(f"   èŠ‚ç‚¹æ€»æ•°: {dist_g.number_of_nodes()}")
# print(f"   è¾¹æ€»æ•°: {dist_g.number_of_edges()}")


# ==========================================================
# 5ï¸âƒ£ æ„å»ºé‡‡æ ·å™¨ä¸æ•°æ®åŠ è½½å™¨
# ==========================================================
# # é‡‡ç”¨é‚»å±…é‡‡æ ·ç­–ç•¥ (2-hop)
# sampler = as_edge_prediction_sampler(
#     NeighborSampler([10, 10]),
#     negative_sampler=dgl.dataloading.negative_sampler.Uniform(1)
# )

# # è¿™é‡Œæˆ‘ä»¬ç®€å•ä½¿ç”¨æ‰€æœ‰è¾¹ç´¢å¼•
# edge_ids = torch.arange(g.num_edges())




# EdgeDataLoader æ”¯æŒå¤šè¿›ç¨‹åˆ†å¸ƒå¼åŠ è½½
# dataloader = EdgeDataLoader(
#     dist_g,
#     edge_ids,
#     sampler,
#     batch_size=batch_size,
#     shuffle=True,
#     num_workers=0
# )

# ==========================================================
# 6ï¸âƒ£ åˆå§‹åŒ– TensorBoard è®°å½•å™¨
# ==========================================================
writer = SummaryWriter(log_dir='runs/link_prediction_experiment')

# ==========================================================
# 7ï¸âƒ£ åˆå§‹åŒ– GNN æ¨¡å‹ï¼ˆé“¾è·¯é¢„æµ‹ä»»åŠ¡ï¼‰
# ==========================================================
# åŸºäºç”Ÿæˆçš„å›¾æ•°æ®ï¼šè¾“å…¥ç»´åº¦128ï¼Œè¾“å‡ºç»´åº¦1ï¼ˆè¾¹å­˜åœ¨æ¦‚ç‡ï¼‰
input_dim = 128  # èŠ‚ç‚¹ç‰¹å¾ç»´åº¦
hidden_dim = 64  # éšè—å±‚ç»´åº¦
output_dim = 1   # ç›´æ¥è¾“å‡ºè¾¹å­˜åœ¨æ¦‚ç‡

# åˆå§‹åŒ–GCNæ¨¡å‹ï¼Œç›´æ¥è¾“å‡ºè¾¹é¢„æµ‹æ¦‚ç‡
gnn = GCN(input_dim, hidden_dim, output_dim, dropout=0.5)
gnn = gnn.to(device)

# ä½¿ç”¨ DistributedDataParallel åŒ…è£…æ¨¡å‹
gnn = torch.nn.parallel.DistributedDataParallel(gnn, device_ids=[local_rank])

# ä¼˜åŒ–å™¨
optimizer = torch.optim.Adam(gnn.parameters(), lr=lr)

print(f"ğŸš€ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ:")
print(f"   è¾“å…¥ç»´åº¦: {input_dim}")
print(f"   éšè—ç»´åº¦: {hidden_dim}")
print(f"   è¾“å‡ºç»´åº¦: {output_dim}")
print(f"   æ€»å‚æ•°é‡: {sum(p.numel() for p in gnn.parameters()):,}")
print(f"   åˆ†å¸ƒå¼è®­ç»ƒ: æ˜¯ | GPUæ•°é‡: {torch.cuda.device_count()}")


# ==========================================================
# 8ï¸âƒ£ è®­ç»ƒå¾ªç¯
# ==========================================================
print("ğŸ¯ å¼€å§‹è®­ç»ƒé“¾è·¯é¢„æµ‹æ¨¡å‹ ...")

# åˆå§‹åŒ–å¸¦å®½ç›‘æ§å˜é‡
prev_comm_time = 0

gnn.train()
for epoch in range(num_epochs):
    total_loss = 0
    num_batches = 0
    epoch_comm_time = 0
    epoch_forward_time = 0
    epoch_backward_time = 0
    
    for input_nodes, pair_graph, blocks in dataloader:
        # é€šä¿¡æ—¶é—´ç›‘æ§å¼€å§‹
        comm_start_time = time.time()
        
        # è·å–èŠ‚ç‚¹ç‰¹å¾
        input_features = dist_g.ndata['feats'][input_nodes].to(device)
        
        # é€šä¿¡æ—¶é—´ç›‘æ§ç»“æŸ
        comm_end_time = time.time()
        comm_time = comm_end_time - comm_start_time
        epoch_comm_time += comm_time
        
        # å‰å‘ä¼ æ’­æ—¶é—´ç›‘æ§
        forward_start_time = time.time()
        node_embeddings = gnn(blocks[0], input_features)
        forward_end_time = time.time()
        forward_time = forward_end_time - forward_start_time
        epoch_forward_time += forward_time
        
        # è·å–è¾¹é¢„æµ‹ç»“æœ - æ‹¼æ¥ä¸¤ä¸ªèŠ‚ç‚¹é‡‡æ ·åçš„ç‰¹å¾ä½œä¸ºè¾“å…¥å‘é‡
        src_features = blocks[0].srcdata['feats'][pair_graph.edges()[0]]
        dst_features = blocks[0].srcdata['feats'][pair_graph.edges()[1]]
        
        # æ‹¼æ¥æºèŠ‚ç‚¹å’Œç›®æ ‡èŠ‚ç‚¹çš„ç‰¹å¾
        combined_features = torch.cat([src_features, dst_features], dim=1)
        
        # ç›´æ¥ä½¿ç”¨GCNè®¡ç®—è¾¹å¾—åˆ†ï¼ˆè¾“å‡ºç»´åº¦ä¸º1ï¼‰
        edge_scores = gnn(pair_graph, combined_features).squeeze(1)
        
        # è·å–æ ‡ç­¾
        edge_labels = pair_graph.edata['label'].to(device).float()
        
        # è®¡ç®—äºŒå…ƒäº¤å‰ç†µæŸå¤±
        loss = nn.functional.binary_cross_entropy_with_logits(
            edge_scores, edge_labels
        )
        
        # åå‘ä¼ æ’­æ—¶é—´ç›‘æ§
        backward_start_time = time.time()
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        backward_end_time = time.time()
        backward_time = backward_end_time - backward_start_time
        epoch_backward_time += backward_time
        
        total_loss += loss.item()
        num_batches += 1
    
    avg_loss = total_loss / num_batches if num_batches > 0 else 0
    avg_comm_time = epoch_comm_time / num_batches if num_batches > 0 else 0
    avg_forward_time = epoch_forward_time / num_batches if num_batches > 0 else 0
    avg_backward_time = epoch_backward_time / num_batches if num_batches > 0 else 0
    
    # è®¡ç®—å¸¦å®½æ³¢åŠ¨ï¼ˆåŸºäºé€šä¿¡æ—¶é—´çš„å˜åŒ–ï¼‰
    if epoch > 0:
        bandwidth_variation = abs(avg_comm_time - prev_comm_time) / prev_comm_time if prev_comm_time > 0 else 0
        writer.add_scalar('Bandwidth/variation', bandwidth_variation, epoch)
    prev_comm_time = avg_comm_time
    
    # è®°å½•åˆ° TensorBoard
    writer.add_scalar('Loss/train', avg_loss, epoch)
    writer.add_scalar('Time/communication', avg_comm_time, epoch)
    writer.add_scalar('Time/forward', avg_forward_time, epoch)
    writer.add_scalar('Time/backward', avg_backward_time, epoch)
    writer.add_scalar('Time/total', avg_comm_time + avg_forward_time + avg_backward_time, epoch)
    
    print(f"ğŸ“Š Epoch [{epoch+1}/{num_epochs}] | å¹³å‡æŸå¤±: {avg_loss:.4f}")
    print(f"   â±ï¸  é€šä¿¡æ—¶é—´: {avg_comm_time:.4f}s | å‰å‘æ—¶é—´: {avg_forward_time:.4f}s | åå‘æ—¶é—´: {avg_backward_time:.4f}s")


# ==========================================================
# 9ï¸âƒ£ æ¸…ç†èµ„æº
# ==========================================================
print("ğŸ§¹ æ¸…ç†åˆ†å¸ƒå¼èµ„æº ...")

# å…³é—­ TensorBoard è®°å½•å™¨
writer.close()

# æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
dist.destroy_process_group()
print("âœ… è®­ç»ƒå®Œæˆï¼")
print("ğŸ“ˆ è®­ç»ƒæ—¥å¿—å·²ä¿å­˜åˆ° 'runs/link_prediction_experiment'ï¼Œä½¿ç”¨ 'tensorboard --logdir=runs' æŸ¥çœ‹")
